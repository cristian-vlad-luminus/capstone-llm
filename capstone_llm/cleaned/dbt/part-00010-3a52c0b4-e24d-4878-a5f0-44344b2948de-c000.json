{"title":"Querying with dbt from external source","question_body":"<p>I have the following issue:</p>\n<ul>\n<li>I have an AWS S3 pipeline that on a daily basis a single json.gz files is spit.</li>\n<li>I wish to take that file with dbt and put it into snowflake (no snowpipe use atm)</li>\n</ul>\n<p>I have managed to do this by creating a storage integration and I have manually created with my role (used for running dbt) a schema and assing usage on that schema. So far so good.</p>\n<p>Then I read about this:</p>\n<p><a href=\"https://github.com/fishtown-analytics/dbt-external-tables\" rel=\"nofollow noreferrer\">https://github.com/fishtown-analytics/dbt-external-tables</a></p>\n<p>Problem is that this is the only way this runs properly, I had to alter my dbt profiles.yml, set the default schema to be S3_MIXPANEL with default database RAW_DEV, run a different target and role on that with --target 'ingest_dev' parameter.</p>\n<p>I keep thinking that there should be a more sophisticated solution, where I can create schema's and query metadata and use something like {{ source() }} so I can point my documentation somehow that this is an external source. This dbt-external-tables is not really well explained for my case here I think?</p>\n<p>Please can anyone help me and share how to create schemas and query from external stages properly without changing default schema macro &amp;  dbtprofiles.yml each time?</p>\n<p>I have succeeded to run the following code:</p>\n<pre><code>{{\n  config(\n    materialized ='incremental',\n    schema = generate_schema_name('S3_MIXPANEL')\n  )\n}}\n \n  SELECT\n    metadata$filename as file_name,\n    to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd') as event_date,\n    $1 as payload,\n    CONVERT_TIMEZONE('Europe/London',TO_TIMESTAMP_tz($1:properties:mp_processing_time_ms::int / 1000)) as  event_timestamp_converted,\n    CONVERT_TIMEZONE('Europe/London', current_timestamp) as ingested_at\n\n from\n\n    @my_s3_stage\n\n    \n{% if is_incremental() %}\n    -- this filter will only be applied on an incremental run\n    WHERE event_date&gt;(\n    SELECT\n        max(event_date)\n    FROM\n        {{ this }}\n    )\n{% endif %}\n\n{{ row_limit() }} \n</code></pre>\n<p>EDIT 22-06-20:</p>\n<p>I have added the src_mixpanel.yml file in my models and ran the dbt command, however I had to also specify the data_types, so I added them too, then I apparently had to add the &quot;macro&quot; in my macros too (btw maybe a stupid question but I don't really know how to install your package, so I manually added all macros from yours into mine).</p>\n<p>Now when I run this code:</p>\n<pre><code>dbt run-operation stage_external_sources\n</code></pre>\n<p>with</p>\n<pre><code>version: 2\n\nsources:\n\n  - name: s3_mixpanel\n    database: RAW_DEV\n    tables:\n      - name: events\n        external:\n          location: '@my_s3_stage'\n          auto_refresh: false # depends on your S3 setup\n          partitions:\n            - name: event_date\n              expression: to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd')\n              data_type: date\n            - name: file_name\n              expression: metadata$filename\n              data_type: string\n          columns:\n            - name: properties\n              data_type: variant\n</code></pre>\n<p>I get an error:</p>\n<blockquote>\n<p>Encountered an error while running operation: Compilation Error in macro stage_external_sources (macros/stage_external_sources.sql)<br />\n'dict object' has no attribute 'sources'</p>\n</blockquote>\n","answer_body":"<p>As the maintainer of the <code>dbt-external-tables</code> package, I'll share its opinionated view. The package believes that you should stage all external sources (S3 files) as external tables or with snowpipes <em>first</em>, in a process that includes as little confounding logic as possible. Then you can select from them, as sources, in dbt models, alongside all requisite business logic.</p>\n<p>If my understanding is correct, you would stage your mixpanel data as below, in a file called (e.g.) <em>models/staging/mixpanel/src_mixpanel.yml</em>:</p>\n<pre><code>version: 2\n\nsources:\n\n  - name: s3_mixpanel\n    database: raw_dev\n    tables:\n      - name: events\n        external:\n          location: '@my_s3_stage'\n          file_format: &quot;( type = json )&quot;  # or a named file format\n          auto_refresh: false # depends on your S3 setup\n          partitions:\n            - name: event_date\n              expression: to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd')\n          columns:\n            - name: properties\n              data_type: variant\n</code></pre>\n<p>You would run this macro from the package to create the external tableâ€”and, after creation, to update its partition metadata if you don't have <code>auto_refresh</code> enabled (see Snowflake <a href=\"https://docs.snowflake.com/en/sql-reference/sql/create-external-table.html\" rel=\"noreferrer\">docs</a>):</p>\n<pre><code>dbt run-operation stage_external_sources\n</code></pre>\n<p>You can then select from this source in an incremental model, like the one you have above. Now, <code>event_date</code> is a partition column on this external table, so filtering on it <strong>should</strong> enable Snowflake to prune files (though that's been inconsistent historically for dynamic, subquery-derived filters).</p>\n<pre><code>{{\n  config(\n    materialized ='incremental'\n  )\n}}\n \n  SELECT\n    metadata$filename as file_name,\n    event_date,\n    value as payload,\n    properties:mp_processing_time_ms::int / 1000 as event_timestamp_converted,\n    CONVERT_TIMEZONE('Europe/London', current_timestamp) as modeled_at\n\n from {{ source('s3_mixpanel', 'events' }} \n\n    \n{% if is_incremental() %}\n    -- this filter will only be applied on an incremental run\n    WHERE event_date &gt;(\n    SELECT\n        max(event_date)\n    FROM\n        {{ this }}\n    )\n{% endif %}\n\n{{ row_limit() }}\n</code></pre>\n"}
