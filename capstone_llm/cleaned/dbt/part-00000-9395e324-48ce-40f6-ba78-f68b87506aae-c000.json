{"title":"Only run a set of models?","question_body":"<p>I started to migrate some of your transformations jobs to DBT. As you can see on the image bellow, there is usually 1 to 2 transformations before to have our final table (up to 5 transformations in some cases).</p>\n<p>What I am trying to achieve is to do dbt run only for a set on linked model. For instance, <code>sales_prediction</code> and <code>forecast</code>.  I am currently able to run either for everything with <code>dbt run o</code>r just speficif model using <code>dbt run --select model_name</code></p>\n<p><a href=\"https://i.sstatic.net/s4jl5.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/s4jl5.png\" alt=\"enter image description here\" /></a></p>\n","answer_body":"<p>In addition to the <a href=\"https://docs.getdbt.com/reference/node-selection/graph-operators#the-plus-operator\" rel=\"noreferrer\">+ operator</a>, incase you just need to run multiple unrelated models by specifying their names, you can do so <a href=\"https://docs.getdbt.com/reference/node-selection/syntax\" rel=\"noreferrer\">as such</a>:</p>\n<pre><code>dbt run --select my_first_model my_second_model\n</code></pre>\n"}
{"title":"Only run a set of models?","question_body":"<p>I started to migrate some of your transformations jobs to DBT. As you can see on the image bellow, there is usually 1 to 2 transformations before to have our final table (up to 5 transformations in some cases).</p>\n<p>What I am trying to achieve is to do dbt run only for a set on linked model. For instance, <code>sales_prediction</code> and <code>forecast</code>.  I am currently able to run either for everything with <code>dbt run o</code>r just speficif model using <code>dbt run --select model_name</code></p>\n<p><a href=\"https://i.sstatic.net/s4jl5.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/s4jl5.png\" alt=\"enter image description here\" /></a></p>\n","answer_body":"<p>Dbt allows syntax of</p>\n<ul>\n<li>selecting a node <em>and all nodes it requires</em> (<code>+</code> before the model name)</li>\n<li>selecting a node <em>and all nodes that depend on it</em> (<code>+</code> after the model name)</li>\n<li>you can also do both (<code>+model_name+</code>)</li>\n</ul>\n<p>In your case <code>dbt run --select +forecast</code> should do the trick</p>\n<p>Also check the <a href=\"https://docs.getdbt.com/reference/node-selection/graph-operators#the-plus-operator\" rel=\"noreferrer\">documentation of <em>the <code>+</code> operator</em></a>.</p>\n"}
{"title":"dbt ref() vs source()","question_body":"<p>I’m trying to make one model in dbt depend on another (trying to run the second model after the first one is completely finished), but I’m confused, when to use <code>ref()</code> or <code>source()</code>?</p>\n<p>What is the difference between them?</p>\n","answer_body":"<p>Using <code>ref</code> creates the lineage for your DAG and will run the predecessor models. Using <code>source</code> references a base table that is not necessarily a model. Rule of thumb is use <code>source</code> in your base models and everything else should use <code>ref</code>.</p>\n<p>Example - Green nodes represent tables ingested into your DWH. The blue/red nodes are DBT models.\n<a href=\"https://i.sstatic.net/n7R7I.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/n7R7I.png\" alt=\"enter image description here\" /></a></p>\n"}
{"title":"Can dbt connect to different databases in the same project?","question_body":"<p>It seems dbt only works for a single database.</p>\n<p>If my data is in a different database, will that still work? For example, if my datalake is using delta, but I want to run dbt using Redshift, would dbt still work for this case?</p>\n","answer_body":"<p>To use dbt, you need to already be able to <code>select from</code> your raw data in your warehouse.</p>\n<p>In general, dbt is not an ETL tool:</p>\n<blockquote>\n<p><em>[dbt] doesn’t extract or load data, but it’s extremely good at transforming data that’s already loaded into your warehouse. This “transform after load” architecture is becoming known as ELT (extract, load, transform). <strong>dbt is the T in ELT.</strong> [<a href=\"https://blog.getdbt.com/what--exactly--is-dbt-/\" rel=\"noreferrer\">reference</a>]</em></p>\n</blockquote>\n<p>So no, you cannot use dbt with Redshift and Deltalake at the same time. Instead, use a separate service to extract and load data into your Redshift cluster — dbt is agnostic about which tool you use to do this.</p>\n<p>There is a nuance to this answer - you <em>could</em> use dbt to select from external files in S3 or GCS, so long as you've set up your data warehouse to be able to read those files. For Redshift, this means setting up <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html\" rel=\"noreferrer\">Redshift Spectrum</a>. (For Snowflake, this means setting up an <a href=\"https://docs.snowflake.com/en/sql-reference/sql/create-external-table.html\" rel=\"noreferrer\">external table</a> and on BigQuery, you can also <a href=\"https://cloud.google.com/bigquery/external-data-cloud-storage\" rel=\"noreferrer\">query cloud storage data</a>)</p>\n<p>So, if the data you read in Deltalake lives in S3, if you set up your Redshift cluster to be able to read it, you can use dbt to transform the data!</p>\n"}
{"title":"How to setup dbt UI for data lineage?","question_body":"<p>I'm new to the dbt and I'm planning to use dbt cli. One question is how to setup the dbt ui and have such a data lineage graph? I didn't find how to do it here with cli <a href=\"https://docs.getdbt.com/tutorial/create-a-project-dbt-cli\" rel=\"noreferrer\">https://docs.getdbt.com/tutorial/create-a-project-dbt-cli</a>.</p>\n<p><a href=\"https://i.sstatic.net/xrBa2.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/xrBa2.png\" alt=\"enter image description here\" /></a></p>\n","answer_body":"<p>dbt docs might be what you're looking for?</p>\n<p>You need to first generate the docs using:</p>\n<p><code> dbt docs generate</code></p>\n<p>Then, serve them:</p>\n<p><code>dbt docs serve</code></p>\n<p>You'll find that the docs are served locally. Once you open the link in a browser you can see the lineage at the bottom right like so:</p>\n<p><a href=\"https://i.sstatic.net/rDhEZ.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/rDhEZ.png\" alt=\"enter image description here\" /></a></p>\n<p>There is more detail here: <a href=\"https://docs.getdbt.com/reference/commands/cmd-docs\" rel=\"noreferrer\">https://docs.getdbt.com/reference/commands/cmd-docs</a></p>\n"}
{"title":"Write dbt test for positive values","question_body":"<p>Is there an easy way to write a test for a column being positive in dbt?</p>\n<p><code>accepted_values</code> doesn't seem to work for continuous variables.</p>\n<p>I know you can write queries in <code>./tests</code> but it looks like an overkill for such a simple thing.</p>\n","answer_body":"<p>Previous answer is correct. Another option is <a href=\"https://github.com/dbt-labs/dbt-utils#accepted_range-source\" rel=\"noreferrer\">accepted_range</a>:</p>\n<pre><code>version: 2\n\nmodels:\n  - name: model_name\n    columns:\n      - name: user_id\n        tests:\n          - dbt_utils.accepted_range:\n              min_value: 0\n              inclusive: false\n</code></pre>\n"}
{"title":"Write dbt test for positive values","question_body":"<p>Is there an easy way to write a test for a column being positive in dbt?</p>\n<p><code>accepted_values</code> doesn't seem to work for continuous variables.</p>\n<p>I know you can write queries in <code>./tests</code> but it looks like an overkill for such a simple thing.</p>\n","answer_body":"<p>You could use <a href=\"https://github.com/dbt-labs/dbt-utils#expression_is_true-source\" rel=\"nofollow noreferrer\"><code>dbt_utils.expression_is_true</code></a></p>\n<pre><code>version: 2\n\nmodels:\n  - name: model_name\n    tests:\n      - dbt_utils.expression_is_true:\n          expression: &quot;col_a &gt; 0&quot;\n</code></pre>\n"}
{"title":"while running the dbt run command gets error","question_body":"<p>connection to dbt and snowfalke was successful but when tried to run this command:</p>\n<pre><code>$ dbt run\n</code></pre>\n<p>it gives this error</p>\n<blockquote>\n<p>ERROR: Runtime Error\nCould not find profile named 'learn_dbt'\nEncountered an error:\nRuntime Error\nCould not run dbt&quot;</p>\n</blockquote>\n<p>Am I making any command mistake?</p>\n","answer_body":"<p>There are a few different approaches to solving to this problem:</p>\n<ol>\n<li>Check the profile key in your <code>dbt_project.yml</code></li>\n<li>Check the profiles you have in your <code>profiles.yml</code></li>\n<li>Run <code>dbt debug --config-dir</code> to check where dbt thinks your config file is.</li>\n</ol>\n<p>See the dbt documentation <a href=\"https://docs.getdbt.com/docs/guides/debugging-errors#could-not-find-profile\" rel=\"noreferrer\">here</a></p>\n"}
{"title":"How do we define select statement as a variable in dbt?","question_body":"<p>Hi I am trying to define a select statement in a set variable in dbt,\ncan any one suggest how to set sql query as a variable in dbt  and how to access those variables in below CTEs?</p>\n","answer_body":"<p>You can use <code>call statement</code> and get the result in a variable with <code>load_result</code></p>\n<p>Here is an example of retrieving only one field from the select statement:</p>\n<pre><code>{%- call statement('my_statement', fetch_result=True) -%}\n      SELECT my_field FROM my_table\n{%- endcall -%}\n\n{%- set my_var = load_result('my_statement')['data'][0][0] -%}\n</code></pre>\n<p>Then you can use <code>{{ my_var }}</code></p>\n<p>You can play with <code>['data'][0][0]</code> depending on the rows and columns that your select returns</p>\n"}
{"title":"Using multiple columns in a Unique_Key for incremental loading in DBT","question_body":"<p>For incremental models, the DBT documentation <a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/building-models/configuring-incremental-models\" rel=\"noreferrer\">here</a> says:</p>\n<blockquote>\n<p>The unique_key should be supplied in your model definition as a string representing a simple column or a list of single quoted column names that can be used together, for example, ['col1', 'col2', …])</p>\n</blockquote>\n<p>I've built an incremental model in DBT with this incremental definition</p>\n<pre><code>{{\n  config(\n    materialized='incremental',\n    unique_key = ['Col1', 'Col2', 'Col3']\n  )\n}}\n</code></pre>\n<p>Which compiles into this merge statement in in Snowflake:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>using DW_DEV.dbt_dgarrison_DATA_STAGING.MY_TABLE__dbt_tmp as DBT_INTERNAL_SOURCE\n    on \n        DBT_INTERNAL_SOURCE.['Col1', 'Col2', 'Col3'] = DBT_INTERNAL_DEST.['Col1', 'Col2', 'Col3']\n...\n</code></pre>\n<p>And this reasonably throws a SQL ERROR complaining about the brackets:</p>\n<blockquote>\n<p>SQL compilation error: syntax error line 4 at position 32 unexpected '['. syntax error line 4 at position 45 unexpected ','. syntax error line 4 at position 98 unexpected '['. syntax error line 4 at position 111 unexpected ','.</p>\n</blockquote>\n<p>I can't find any other good examples using multiple columns this way. (there are options involving concatenating columns, and I'm open to recommendations on the best approach to that, but I'm trying to figure out how to use the DBT recommended syntax)</p>\n","answer_body":"<p>As part of <a href=\"https://github.com/dbt-labs/dbt-core/releases/tag/v1.1.0\" rel=\"noreferrer\">dbt-core 1.1.0</a>, we can now <a href=\"https://github.com/dbt-labs/dbt-core/pull/4618\" rel=\"noreferrer\">pass a list to the unique_key statement in incremental models</a>. See the original issue <a href=\"https://github.com/dbt-labs/dbt-core/issues/2479\" rel=\"noreferrer\">here</a>.</p>\n<p>This means that you should be able to achieve your goal by updating <code>dbt-core</code> and your <code>dbt-&lt;adapter&gt;</code> version locally; or updating your dbt Cloud version accordingly, to 1.1.0, since given the error you get, it looks like <code>unique_key</code> is still looking for a single string instead of an array.</p>\n"}
{"title":"What is the .user.yml file? What is its purpose?","question_body":"<p>I'm starting with DBT. When I ran my project it created a <code>.user.yml</code> file. Its content:</p>\n\n<pre><code>{id: c8e8abd2-09a3-4699-b444-3ef7ee5b04e5}\n</code></pre>\n\n<p>It seems from <a href=\"https://github.com/fishtown-analytics/dbt/issues/1645\" rel=\"noreferrer\">this github issue</a> that it's some kind of cookie, but I could not find any info anywhere on what its role is.</p>\n\n<p>Can someone explain the purpose of this file? Should I add it to my <code>.gitignore</code>?</p>\n","answer_body":"<p>Just got answered on the dbt slack.</p>\n<p>The file just contains a cookie for dbt's anonymous usage tracking. It happened to land in my repo because I placed my <code>profiles.yml</code> in my repo. (my credentials are in environment variables). But normally it's rather created in <code>~/.dbt</code></p>\n"}
{"title":"Using if block in dbt models","question_body":"<p>Apologies for asking dumb question. But I tried many different approaches but none of them seems to work.</p>\n<p>I have a requirement to select data from 2 different tables based on the variable. I am trying to do that in dbt models with if statement but it doesn't seem to work.</p>\n<p>Model looks something like thins:</p>\n<pre><code>SELECT \n*\nFROM\n{% if enable_whitelisting == 'true' %}\n    {{ ref('accounts_whitelisted') }}    accounts\n{% else %}\n        {{ ref('accounts') }}   accounts\n{% endif %}\n</code></pre>\n<p>Any help is appreciated.</p>\n<p>Thanks in advance.</p>\n","answer_body":"<p>I got this working eventually. Have to put the variable name within the <strong>var()</strong></p>\n<pre><code>SELECT \n*\nFROM\n{% if var('enable_whitelisting') == 'true' %}\n    {{ ref('accounts_whitelisted') }}    accounts\n{% else %}\n        {{ ref('accounts') }}   accounts\n{% endif %}\n</code></pre>\n"}
{"title":"DBT - environment variables and running dbt","question_body":"<p>I am relatively new to DBT and I have been reading about <code>env_var</code> and I want to use this in a couple of situations and I am having difficultly and looking for some support.</p>\n<p>firstly I am trying to use it in my profiles.yml file to replace the user and password so that this can be set when it is invoked. When trying to test this locally (before implementing this on our AWS side) I am failing to find the right syntax and not finding anything useful online.\nI have tried variations of:</p>\n<pre><code>dbt run --vars '{DBT_USER: my_username, DBT_PASSWORD=my_password}'\n</code></pre>\n<p>but it is not recognizing and giving nothing useful error wise. When running dbt run by itself it does ask for <code>DBT_USER</code> so it is expecting it, but doesn't detail how</p>\n<p>I would also like to use it in my <code>dbt_project.yml</code> for the schema but I assume that this will be similar to the above, just a third variable at the end. Is that the case?</p>\n<p>Thanks</p>\n","answer_body":"<p><code>var</code> and <code>env_var</code> are two separate features of dbt.</p>\n<p>You can use <code>var</code> to access a global variable you define in your <code>dbt_project.yml</code> file. The <code>--vars</code> command-line option lets you override the values of these vars at runtime. See the <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/var\" rel=\"nofollow noreferrer\">docs for <code>var</code></a>.</p>\n<p>You should use <code>env_var</code> to access <a href=\"https://www.techrepublic.com/article/linux-101-what-are-environment-variables/\" rel=\"nofollow noreferrer\">environment variables</a> that you set outside of dbt for your system, user, or shell session. Typically you would use environment variables to store secrets like your profile's connection credentials.</p>\n<p>To access environment variables in your <code>profiles.yml</code> file, you replace the values for username and password with a call to the <code>env_var</code> macro, as they do in the <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/env_var\" rel=\"nofollow noreferrer\">docs for <code>env_var</code></a>:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>profile:\n  target: prod\n  outputs:\n    prod:\n      type: postgres\n      host: 127.0.0.1\n      # IMPORTANT: Make sure to quote the entire Jinja string here\n      user: &quot;{{ env_var('DBT_USER') }}&quot;\n      password: &quot;{{ env_var('DBT_PASSWORD') }}&quot;\n      ....\n</code></pre>\n<p>Then BEFORE you issue the <code>dbt_run</code> command, you need to set the <code>DBT_USER</code> and <code>DBT_PASSWORD</code> environment variables for your system, user, or shell session. This will depend on your OS, but there are lots of good instructions on this. To set a var for your shell session (for Unix OSes), that could look like this:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>$ export DBT_USER=my_username\n$ export DBT_PASSWORD=abc123\n$ dbt run\n</code></pre>\n<p>Note that storing passwords in environment variables isn't necessarily more secure than keeping them in your <code>profiles.yml</code> file, since they're stored in plaintext and not protected from being dumped into logs, etc. (You shouldn't be checking <code>profiles.yml</code> into source control). You should consider at least using an environment variable name prefixed by <code>DBT_ENV_SECRET_</code> so that dbt keeps them out of logs. See <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/env_var#secrets\" rel=\"nofollow noreferrer\">the docs</a> for more info</p>\n"}
{"title":"Materialized View vs Table Using dbt","question_body":"<p>I'm just onboarding dbt and having gone through the tutorial docs I'm wondering if there's a difference between materializing my transformations as views or tables? I'm using Snowflake as the data warehouse. There's some documentation <a href=\"https://docs.snowflake.com/en/user-guide/views-materialized.html#comparison-with-tables-regular-views-and-cached-results\" rel=\"noreferrer\">here</a> that shows the differences between a table and a materialized view but if I'm using dbt to update the tables regularly, do they more or less become the same thing?</p>\n<p>Thanks!</p>\n","answer_body":"<p>dbt doesn't support materialized views, as far as I'm aware, but as Felipe commented, there is an <a href=\"https://github.com/fishtown-analytics/dbt/issues/1162\" rel=\"noreferrer\">open issue to discuss it</a>. If it <em>were</em> possible to use materialized views on Snowflake, you're right that they <em>somewhat</em> become the same thing. The materialized view would update even if you haven't run dbt. As Drew mentions in the ticket though, there are a lot of caveats that make using tables with dbt preferable in most use cases: &quot;no window functions, no unions, limited aggregates, can't query views, etc etc etc&quot;.</p>\n<p>That said, dbt does support views and tables.</p>\n<p>Even when you're using dbt, there's still a difference between a view and a table. A table will always need to be refreshed by dbt in order to be updated. A view will always be as up-to-date as the underlying tables it is referencing.</p>\n<p>For example, let's say you have a dbt model called <code>fct_orders</code> which references a table that is loaded by Fivetran/Stitch called <code>shopify.order</code>. If your model is materialized as a view, it will always return the most up-to-date data in the Shopify table. If it is materialized as a table, and new data has arrived in the Shopify table since you last run dbt, the model will be 'stale'.</p>\n<p>That said, the benefit of materializing it as a table is that it will run more quickly, given it's not having to do the SQL 'transformation' each time.</p>\n<p>The advice I have seen given most often is something like this:</p>\n<ul>\n<li>If using a view isn't too slow for your end-users, use a view.</li>\n<li>If a view gets too slow for your end-users, use a table.</li>\n<li>If building a table with dbt gets too slow, use incremental models in dbt.</li>\n</ul>\n"}
{"title":"DBT - [WARNING]: Did not find matching node for patch","question_body":"<p>I keep getting the error below when I use <em>dbt run</em> - I can't find anything on why this error occurs or how to fix it within the dbt documentation.</p>\n<pre><code>[WARNING]: Did not find matching node for patch with name 'vGenericView' in the 'models' section of file 'models\\generic_schema\\schema.sql'\n</code></pre>\n","answer_body":"<p>did you by chance recently upgrade to dbt 1.0.0? If so, this means that you have a model, <code>vGenericView</code> defined in a <code>schema.yml</code> but you don't have a <code>vGenericView.sql</code> model file to which it corresponds.</p>\n"}
{"title":"How do I run DBT models from a Python script or program?","question_body":"<p>I have a DBT project, and a python script will be grabbing data from the postgresql to produce output.</p>\n<p>However, part of the python script will need to make the DBT run. I haven't found the library that will let me cause a DBT run from an external script, but I'm pretty sure it exists. How do I do this?</p>\n<p>ETA: The correct answer may be to download the DBT CLI and then use python system calls to use that.... I was hoping for a library, but I'll take what I can get.</p>\n","answer_body":"<h2>Update: v1.5 has arrived!</h2>\n<p>With v1.5 of dbt, we get a stable and officially supported Python API for invoking dbt operations; this API has functional parity with the CLI.</p>\n<p>From the <a href=\"https://docs.getdbt.com/reference/programmatic-invocations\" rel=\"noreferrer\">docs</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from dbt.cli.main import dbtRunner, dbtRunnerResult\n\n# initialize\ndbt = dbtRunner()\n\n# create CLI args as a list of strings\ncli_args = [&quot;run&quot;, &quot;--select&quot;, &quot;tag:my_tag&quot;]\n\n# run the command\nres: dbtRunnerResult = dbt.invoke(cli_args)\n\n# inspect the results\nfor r in res.result:\n    print(f&quot;{r.node.name}: {r.status}&quot;)\n</code></pre>\n<p>There are some caveats about the stability of artifacts returned by <code>dbt.invoke</code>; read the docs for more details.</p>\n<h2>Original Answer</h2>\n<p>(As of Jan 2023) There is not a public Python API for dbt, yet. It is expected in v1.5, which should be out in a couple months.</p>\n<p>Right now, your safest option is to use the CLI. If you don't want to use <code>subprocess</code>, the CLI uses <a href=\"https://click.palletsprojects.com/en/8.1.x/\" rel=\"noreferrer\">Click</a> now, and Click provides a <a href=\"https://click.palletsprojects.com/en/8.1.x/api/#click.testing.CliRunner\" rel=\"noreferrer\">runner</a> that you can use to invoke Click commands. It's usually used for testing, but I think it would work for your use case, too. The CLI command is <a href=\"https://github.com/dbt-labs/dbt-core/blob/7077c475511b4f251245b6e246b9a23bed18eed0/core/dbt/cli/main.py#L292\" rel=\"noreferrer\">here</a>. That would look something like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>from click.testing import CliRunner\nfrom dbt.cli.main import run\n\ndbt_runner = CliRunner()\ndbt_runner.invoke(run, args=&quot;-s my_model&quot;)\n</code></pre>\n<p>You could also invoke dbt the way they do in the test suite, using <a href=\"https://github.com/dbt-labs/dbt-core/blob/1e35339389ded85631128b66f57c62ca60649c88/core/dbt/tests/util.py#L63\" rel=\"noreferrer\"><code>run_dbt</code></a>.</p>\n"}
{"title":"Can dbt macros accept other macros as arguments?","question_body":"<p>I am curious if I can pass macros into another macro like this:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>{% macro my_macro(a, b, another_macro) %}\n  ...\n  {{ another_macro(a,b) }}\n  ...\n{% endmacro %}\n</code></pre>\n<p><strong>BONUS:</strong>\nIf dbt's framework can allow it am able to how can I pass arguments to it?</p>\n<p>In R it would look like</p>\n<pre><code>my_callable_function &lt;- function(another_function, ...) {\n  another_function(...)\n}\n</code></pre>\n","answer_body":"<p>A conversation on dbt cloud's <a href=\"https://getdbt.slack.com/archives/C2JRRQDTL/p1630952469173300\" rel=\"noreferrer\">slack</a> and a bit of poking and prodding yielded me the answer.</p>\n<p><strong>Yes</strong> you can pass nested macros into a macro much like nested functions in different languages!</p>\n<p>An example could look like this!</p>\n<pre><code>{% macro base_macro(func1, arg1, arg2) %}\n  {{ func1(arg1, arg2) }}\n{% endmacro %}\n</code></pre>\n"}
{"title":"How to run multi-tag selector","question_body":"<p>I am using dbt 0.18.1 and I follow the documentation about tags however I am curious to know how to run multi-tag selector as arguments.\nAccording to this:\n<a href=\"https://github.com/fishtown-analytics/dbt/pull/1014\" rel=\"noreferrer\">https://github.com/fishtown-analytics/dbt/pull/1014</a></p>\n<blockquote>\n<p>Select using a mix of tags, fqns, and parent/child selectors:\n$ dbt run --model tag:nightly+ salesforce.*+</p>\n</blockquote>\n<p>Unfortunately this is not really a &quot;mix of tags&quot;. <br/><br/>\nI have tags of [mixpanel_tests, quality] and I wish to run models that have both tags included (not separated). If I run <code>dbt run -m tag:quality -t blabla</code></p>\n<ol>\n<li>I would have executed all models that have QUALITY in the array of tags regardless if its single argument or multiple argument however I wish to run ONLY quality marked. How to do that?</li>\n<li>How do I specify 2 tags or 3 tags selector to run models with the mentioned tags (i.e mixpanel_tests, quality - but only those models that have both tags defined). More or less an AND clause rather than an OR clause.\n<br/>\nHmm I hope it is clear. How to have multitag selector that executes only the combination of tags given?</li>\n</ol>\n","answer_body":"<p>Check out the <a href=\"https://docs.getdbt.com/reference/node-selection/set-operators/#intersections\" rel=\"noreferrer\">intersection operator</a>. It's new in dbt v0.18, and it's for this use case exactly.</p>\n<pre><code>dbt run -m tag:mixpanel_tests,tag:quality\n</code></pre>\n"}
{"title":"dbt to snowflake connections fails via profiles.yml","question_body":"<p>I'm trying to connect to snowflake via dbt but connections fail with the error below:</p>\n<pre><code>Using profiles.yml file at /home/myname/.dbt/profiles.yml\nUsing dbt_project.yml file at /mnt/c/Users/Public/learn_dbt/rks-learn-dbt/learn_dbt/dbt_project.yml\nConfiguration:\n  profiles.yml file [ERROR invalid]\n  dbt_project.yml file [OK found and valid]\nProfile loading failed for the following reason:\nRuntime Error\n  Could not find profile named 'learn_dbt'\nRequired dependencies:\n - git [OK found] \n</code></pre>\n<p>Any advice please.</p>\n<p>Note: I am learning to setup dbt connections looking at udemy videos.</p>\n<p>Below is my <code>profiles.yml</code> file:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>learn_dbt:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: XXXXXX\n      user: XXXX                \n      password: XXXX                     \n      role: transform_role\n      database: analytics\n      warehouse: transform_wh\n      schema: dbt\n      threads: 1\n      client_session_keep_alive: False\n</code></pre>\n","answer_body":"<p>My first guess is that you have a <code>profiles.yml</code> file in your dbt project folder and dbt is not actually using the one in <code>/home/myname/.dbt/</code>.</p>\n<p>Could you try running the following?</p>\n<pre class=\"lang-sh prettyprint-override\"><code>dbt debug --profiles-dir /home/myname/.dbt\n</code></pre>\n<p>The flag <code>--profiles-dir</code> works on most dbt cli commands and lets you use a custom <code>profiles.yml</code> that's outside your project. I use this flag all the time.</p>\n"}
{"title":"Running DBT within Airflow through the Docker Operator","question_body":"<p>Building my question on <a href=\"https://stackoverflow.com/questions/64890144/how-to-run-dbt-in-airflow-without-copying-our-repo\">How to run DBT in airflow without copying our repo</a>, I am currently running airflow and syncing the dags via git. I am considering different option to include DBT within my workflow. One suggestion by <a href=\"https://stackoverflow.com/users/3823815/louis-guitton\">louis_guitton</a> is to Dockerize the DBT project, and run it in Airflow via the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html\" rel=\"noreferrer\">Docker Operator</a>.</p>\n<p>I have no prior experience using the Docker Operator in Airflow or generally DBT. I am wondering if anyone has tried or can provide some insights about their experience incorporating that workflow, my main questions are:</p>\n<ol>\n<li>Should DBT as a whole project be run as one Docker container, or is it broken down? (for example: are tests ran as a separate container from dbt tasks?)</li>\n<li>Are logs and the UI from DBT accessible and/or still useful when run via the Docker Operator?</li>\n<li>How would partial pipelines be run? (example: wanting to run only a part of the pipeline)</li>\n</ol>\n","answer_body":"<p>Judging by your questions, you would benefit from trying to dockerise dbt on its own, independently from airflow. A lot of your questions would disappear. But here are my answers anyway.</p>\n<ol>\n<li>\n<blockquote>\n<p>Should DBT as a whole project be run as one Docker container, or is it broken down? (for example: are tests ran as a separate container from dbt tasks?)</p>\n</blockquote>\n</li>\n</ol>\n<p>I suggest you build one docker image for the entire project. The docker image can be based on the python image since dbt is a python CLI tool. You then use the CMD arguments of the docker image to run any dbt command you would run outside docker.\nPlease remember the syntax of <code>docker run</code> (which has nothing to do with dbt): you can specify any COMMAND you wand to run at invocation time</p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\n</code></pre>\n<p>Also, the first hit on Google for &quot;docker dbt&quot; is <a href=\"https://github.com/davidgasquez/dbt-docker/blob/master/Dockerfile\" rel=\"noreferrer\">this dockerfile</a> that can get you started</p>\n<ol start=\"2\">\n<li>\n<blockquote>\n<p>Are logs and the UI from DBT accessible and/or still useful when run via the Docker Operator?</p>\n</blockquote>\n</li>\n</ol>\n<p>Again, it's not a dbt question but rather a docker question or an airflow question.</p>\n<p>Can you see the logs in the airflow UI when using a DockerOperator? Yes, <a href=\"https://marclamberti.com/blog/how-to-use-dockeroperator-apache-airflow/\" rel=\"noreferrer\">see this how to blog post with screenshots</a>.</p>\n<p>Can you access logs from a docker container? Yes, Docker containers emit logs to <code>stdout</code> and <code>stderr</code> output streams (which you can see in airflow, since airflow picks this up). But logs are also stored in JSON files on the host machine in a folder <code>/var/lib/docker/containers/</code>. If you have any advanced needs, you can pick up those logs with a tool (or a simple BashOperator or PythonOperator) and do what you need with it.</p>\n<ol start=\"3\">\n<li>\n<blockquote>\n<p>How would partial pipelines be run? (example: wanting to run only a part of the pipeline)</p>\n</blockquote>\n</li>\n</ol>\n<p>See answer 1, you would run your docker dbt image with the command</p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run my-dbt-image dbt run -m stg_customers\n</code></pre>\n"}
{"title":"How to run DBT in airflow without copying our repo","question_body":"<p>We use airflow to orchestrate our workflows, and dbt with bigquery for our daily transformations in BigQuery. We have two separate git repos, one for our dbt project and a separate one for airflow.</p>\n<p>It seems the simplest approach to scheduling our daily <code>run dbt</code> seems to be a <code>BashOperator</code> in airflow. However, to schedule DBT to run with Airflow, it seems like our entire DBT project would need to be nested inside of our Airflow project, that way we can point to it for our <code>dbt run</code> bash command?</p>\n<p>Is it possible to trigger our <code>dbt run</code> and <code>dbt test</code> without moving our DBT directory inside of our Airflow directory? With the <a href=\"https://github.com/gocardless/airflow-dbt\" rel=\"nofollow noreferrer\">airflow-dbt package</a>,  for the <code>dir</code> in the <code>default_args</code>, maybe it is possible to point to the gibhub link for the DBT project here?</p>\n","answer_body":"<p>My advice would be to leave your dbt and airflow codebases separated.\nThere is indeed a better way:</p>\n<ol>\n<li>dockerise your dbt project in a simple python-based image where you COPY the codebase</li>\n<li>push that to DockerHub or ECR or any other docker repository that you are using</li>\n<li>use the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html\" rel=\"noreferrer\"><code>DockerOperator</code></a> in your airflow DAG to run that docker image with your dbt code</li>\n</ol>\n<p>I'm assuming that you use the airflow LocalExecutor here and that you want to execute your <code>dbt run</code> workload on the server where airflow is running. If that's not the case and that you have access to a Kubernetes cluster, I would suggest instead to use the <code>KubernetesPodOperator</code>.</p>\n"}
{"title":"will DBT support temp table creation like create table #temp1 as select * from tab1 or it works only CTE way","question_body":"<p>I found out a way to handle the temp tables in DBT, write all those in pre-hook and call the final temp table in the outside of the pre-hook, tested and is working fine, able to reduce the code running time from more than 20 mins to 1 min. But I see one problem that we can't see the lineage graph in the DBT documents.\nIs there any way to handle the temp tables other than pre-hook and with lineage in Docs?</p>\n","answer_body":"<p>You're right in thinking that dbt does not support temporary tables. That's because temporary tables only persist in a single session, and dbt opens one connection/session per thread. Therefore any temporary tables created on one thread would not be visible to a model running on a different thread.</p>\n<p>It sounds like CTEs are a performance drag for you though — out of interest, which warehouse are you using?</p>\n<p>You've identified two workarounds, and there's another one worth discussing:</p>\n<p><strong>Option 1: Materialize your model as CTEs using the <code>ephemeral</code> materialization (<a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/building-models/materializations/#ephemeral\" rel=\"noreferrer\">docs</a>)</strong></p>\n<p><em>Pros:</em></p>\n<ul>\n<li>The models show up in the lineage graph</li>\n<li>You can re-use these transformations in multiple downstream models by <code>ref</code>-ing them</li>\n<li>You can test and document these models</li>\n</ul>\n<p><em>Cons:</em></p>\n<ul>\n<li>At some point there is a performance degradation with too many stacked CTEs (especially on older versions of postgres, where CTEs are an optimization fence)</li>\n<li>Compiled SQL can be harder to debug</li>\n</ul>\n<p><strong>Option 2: Use pre-hooks to create temp tables</strong></p>\n<p>I would generally recommend against this — you can't test or document your models, and they won't be in the lineage graph (as you've noted).</p>\n<p><strong>Option 3: Materialize these models as tables in a separate schema, and drop the schema at the end of a run</strong></p>\n<p>I think Michael's suggestion is a good one! I'd tweak it just a little bit:</p>\n<ol>\n<li>Use the <a href=\"https://docs.getdbt.com/reference/resource-configs/schema/\" rel=\"noreferrer\">schema</a> config to materialize a model in a separate schema</li>\n</ol>\n<pre><code>{{ config(\n  materialized='table',\n  schema='my_temporary_schema'\n) }}\n</code></pre>\n<ol start=\"2\">\n<li>Then, at the end of a run, use an <code>on-run-end</code> hook (<a href=\"https://docs.getdbt.com/reference/project-configs/on-run-start-on-run-end/\" rel=\"noreferrer\">docs</a>) to drop that schema — in your <code>dbt_project.yml</code>:</li>\n</ol>\n<pre><code>on-run-end: &quot;drop schema my_temporary_schema cascade&quot;\n</code></pre>\n<p><em>Pros:</em></p>\n<ul>\n<li>All the benefits of Option 1</li>\n<li>Sounds like it might be more performant than using CTEs</li>\n</ul>\n<p><em>Cons:</em></p>\n<ul>\n<li>Make sure you don't have any dependent views on top of that schema! They might get dropped when you run a <code>drop cascade</code> command! This introduces fragility into your project!</li>\n</ul>\n"}
{"title":"dbt relationship test compilation error: test definition dictionary must have exactly one key","question_body":"<p>I'm a new user of dbt, trying to write a relationship test:</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>- name: PROTOCOL_ID\n  tests:\n    - relationships:\n    to: ref('Animal_Protocols')\n    field: id\n</code></pre>\n<p>I am getting this error:</p>\n<pre><code>Compilation Error\nInvalid test config given in models/Animal_Protocols/schema.yml:\ntest definition dictionary must have exactly one key, got [('relationships', None), ('to', &quot;ref('Animal_Protocols')&quot;), ('field', 'id')] instead (3 keys)\n@: UnparsedNodeUpdate(original_file_path='model...ne)\n</code></pre>\n<p>&quot;unique&quot; and &quot;not-null&quot; tests in the same file are working fine, but I have a similar error with &quot;accepted_values&quot;.</p>\n<p>I am using dbt cli version 0.21.0 with Snowflake on MacOS Big Sur 11.6.</p>\n","answer_body":"<p>You are very close! I'm 96% sure that this is an indentation issue -- the #1 pain point of working with YAML. The solution is that both <code>to</code> and <code>field</code> need to be indented below the <code>relationships</code> key as opposed to at the same level.</p>\n<p>See the <a href=\"https://docs.getdbt.com/docs/building-a-dbt-project/tests/#generic-tests\" rel=\"noreferrer\">Tests dbt docs page</a> for an example</p>\n<pre class=\"lang-yaml prettyprint-override\"><code>  - name: PROTOCOL_ID\n    tests:\n      - relationships:\n          to: ref('Animal_Protocols')\n          field: id\n</code></pre>\n"}
{"title":"Add index to dbt model column?","question_body":"<p>We are considering using dbt to manage models in our PostgreSQL data warehouse. Since dbt models are SQL select statements, there doesn't seem to be an obvious, or documented, way to specify that a particular column should have an index.</p>\n<p>How can we specify column indexes on dbt models?</p>\n","answer_body":"<p>From <a href=\"https://docs.getdbt.com/reference/resource-configs/postgres-configs#indexes\" rel=\"noreferrer\">dbt docs</a>:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>{{ config(\n    materialized = 'table',\n    indexes=[\n      {'columns': ['column_a'], 'type': 'hash'},\n      {'columns': ['column_a', 'column_b'], 'unique': True},\n    ]\n)}}\n\nselect ...\n</code></pre>\n"}
{"title":"DBT doesn&#39;t throw an error when running dbt docs generate but the catalog.json is missing","question_body":"<p>I'm new to dbt, I am successfully able to create my models and schemas and macros, but when I do dbt docs generate I get manifest.json, but not catalog.json and so <code>dbt docs serve</code> is failing. It throws the below error.</p>\n<p><a href=\"https://i.sstatic.net/WPPrB.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/WPPrB.png\" alt=\"enter image description here\" /></a></p>\n<p>I checked through my logs I don't find an error, I ran the generated sql from logs in snowflake and it works as well.</p>\n<p>Below is what I get when I run dbt docs generate</p>\n<pre><code>(ENV) C:\\Users\\grands1\\Desktop\\Projects\\Project_name&gt; dbt docs generate\n18:45:10  Running with dbt=1.0.4\n18:45:10  Found 9 models, 4 tests, 0 snapshots, 0 analyses, 180 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n18:45:10\n18:45:15  Concurrency: 1 threads (target='dev')\n18:45:15\n18:45:16  Done.\n18:45:16  Building catalog\n(ENV) C:\\Users\\grands1\\Desktop\\Projects\\Project_name&gt; \n\n</code></pre>\n<p>Is there a way to understand where this issue is coming from? Is it possible to better debug dbt docs generate command?</p>\n<p>Thank you,\nSai.</p>\n","answer_body":"<p>In my case, I had to execute first</p>\n\n<pre><code>dbt docs generate\n</code></pre>\n<p>and then</p>\n\n<pre><code>dbt docs serve\n</code></pre>\n<p>which started localhost with desired docs. It was run under dbt=1.3.0</p>\n"}
{"title":"In dbt, when I add a well formatted .yml file to my project, I stop being able to run dbt or compile SQL until I delete the yaml file. Why?","question_body":"<p>Thank you in advance for helping me in my journey! I am a dbt newb, doing the <a href=\"https://courses.getdbt.com/courses/take/fundamentals/texts/17813360-learning-objectives\" rel=\"noreferrer\">dbt fundamentals course</a>. I am following the directions exactly.</p>\n<p>Sequence of the issue:</p>\n<ol>\n<li><p>I created .sql files in my models folder and subfolders, compiled them, ran dbt, and they showed up in my Snowflake DW. No problem</p>\n</li>\n<li><p>I added a .yml file to one of the sub folders\n<a href=\"https://i.sstatic.net/QLfPl.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/QLfPl.png\" alt=\"enter image description here\" /></a></p>\n</li>\n<li><p>The issue happens when I click &quot;save&quot; to save the code to the .yml file - that the course provided <a href=\"https://courses.getdbt.com/courses/take/fundamentals/texts/17704336-practice\" rel=\"noreferrer\">here</a>. (See compilation error in screenshot)</p>\n</li>\n</ol>\n<p><a href=\"https://i.sstatic.net/MCVAF.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/MCVAF.png\" alt=\"enter image description here\" /></a></p>\n<ol start=\"4\">\n<li>When I click on Compilation Error, I get this (Refreshing the IDE does not help):</li>\n</ol>\n<p>Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.\n'version'</p>\n<p><a href=\"https://i.sstatic.net/Les2W.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Les2W.png\" alt=\"enter image description here\" /></a></p>\n<ol start=\"5\">\n<li><p>At this point, I am not able to run any of the models, even those in other subfolders. For example\n<a href=\"https://i.sstatic.net/5sxOS.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/5sxOS.png\" alt=\"enter image description here\" /></a></p>\n</li>\n<li><p>Also, those .sql files also have the same compilation error above.</p>\n</li>\n<li><p>When I delete the .yml file, everything goes to normal and all the errors disappear.</p>\n</li>\n<li><p>things I have tried:</p>\n</li>\n</ol>\n<ul>\n<li>Deleting all the contents of the yml file and hitting save --&gt; the compilation error goes away</li>\n<li>Changing indentation</li>\n<li>Only leaving a single test instead of two</li>\n<li>Creating the yml file in a different subfolder</li>\n<li>Signing out of dbt and back in</li>\n</ul>\n<p>Please help!\nthank you</p>\n","answer_body":"<p>I think it may be this bug: <a href=\"https://github.com/dbt-labs/dbt/issues/3567\" rel=\"noreferrer\">https://github.com/dbt-labs/dbt/issues/3567</a></p>\n<p>That's related to <a href=\"https://docs.getdbt.com/reference/parsing#partial-parsing\" rel=\"noreferrer\">partial parsing</a>, which is a dbt Core performance optimization used in the dbt Cloud IDE. We rebuilt partial parsing from the ground up in v0.20.0, and we'll be including fixes for bugs we uncover (including that one) in v0.20.1.</p>\n<p>In the meantime, if you find yourself in an error state like the one above, you can trigger a full re-parse by deleting the file <code>target/partial_parse.msgpack</code>.</p>\n"}
{"title":"Querying with dbt from external source","question_body":"<p>I have the following issue:</p>\n<ul>\n<li>I have an AWS S3 pipeline that on a daily basis a single json.gz files is spit.</li>\n<li>I wish to take that file with dbt and put it into snowflake (no snowpipe use atm)</li>\n</ul>\n<p>I have managed to do this by creating a storage integration and I have manually created with my role (used for running dbt) a schema and assing usage on that schema. So far so good.</p>\n<p>Then I read about this:</p>\n<p><a href=\"https://github.com/fishtown-analytics/dbt-external-tables\" rel=\"nofollow noreferrer\">https://github.com/fishtown-analytics/dbt-external-tables</a></p>\n<p>Problem is that this is the only way this runs properly, I had to alter my dbt profiles.yml, set the default schema to be S3_MIXPANEL with default database RAW_DEV, run a different target and role on that with --target 'ingest_dev' parameter.</p>\n<p>I keep thinking that there should be a more sophisticated solution, where I can create schema's and query metadata and use something like {{ source() }} so I can point my documentation somehow that this is an external source. This dbt-external-tables is not really well explained for my case here I think?</p>\n<p>Please can anyone help me and share how to create schemas and query from external stages properly without changing default schema macro &amp;  dbtprofiles.yml each time?</p>\n<p>I have succeeded to run the following code:</p>\n<pre><code>{{\n  config(\n    materialized ='incremental',\n    schema = generate_schema_name('S3_MIXPANEL')\n  )\n}}\n \n  SELECT\n    metadata$filename as file_name,\n    to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd') as event_date,\n    $1 as payload,\n    CONVERT_TIMEZONE('Europe/London',TO_TIMESTAMP_tz($1:properties:mp_processing_time_ms::int / 1000)) as  event_timestamp_converted,\n    CONVERT_TIMEZONE('Europe/London', current_timestamp) as ingested_at\n\n from\n\n    @my_s3_stage\n\n    \n{% if is_incremental() %}\n    -- this filter will only be applied on an incremental run\n    WHERE event_date&gt;(\n    SELECT\n        max(event_date)\n    FROM\n        {{ this }}\n    )\n{% endif %}\n\n{{ row_limit() }} \n</code></pre>\n<p>EDIT 22-06-20:</p>\n<p>I have added the src_mixpanel.yml file in my models and ran the dbt command, however I had to also specify the data_types, so I added them too, then I apparently had to add the &quot;macro&quot; in my macros too (btw maybe a stupid question but I don't really know how to install your package, so I manually added all macros from yours into mine).</p>\n<p>Now when I run this code:</p>\n<pre><code>dbt run-operation stage_external_sources\n</code></pre>\n<p>with</p>\n<pre><code>version: 2\n\nsources:\n\n  - name: s3_mixpanel\n    database: RAW_DEV\n    tables:\n      - name: events\n        external:\n          location: '@my_s3_stage'\n          auto_refresh: false # depends on your S3 setup\n          partitions:\n            - name: event_date\n              expression: to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd')\n              data_type: date\n            - name: file_name\n              expression: metadata$filename\n              data_type: string\n          columns:\n            - name: properties\n              data_type: variant\n</code></pre>\n<p>I get an error:</p>\n<blockquote>\n<p>Encountered an error while running operation: Compilation Error in macro stage_external_sources (macros/stage_external_sources.sql)<br />\n'dict object' has no attribute 'sources'</p>\n</blockquote>\n","answer_body":"<p>As the maintainer of the <code>dbt-external-tables</code> package, I'll share its opinionated view. The package believes that you should stage all external sources (S3 files) as external tables or with snowpipes <em>first</em>, in a process that includes as little confounding logic as possible. Then you can select from them, as sources, in dbt models, alongside all requisite business logic.</p>\n<p>If my understanding is correct, you would stage your mixpanel data as below, in a file called (e.g.) <em>models/staging/mixpanel/src_mixpanel.yml</em>:</p>\n<pre><code>version: 2\n\nsources:\n\n  - name: s3_mixpanel\n    database: raw_dev\n    tables:\n      - name: events\n        external:\n          location: '@my_s3_stage'\n          file_format: &quot;( type = json )&quot;  # or a named file format\n          auto_refresh: false # depends on your S3 setup\n          partitions:\n            - name: event_date\n              expression: to_date(SUBSTR(metadata$filename,16,10),'yyyy/mm/dd')\n          columns:\n            - name: properties\n              data_type: variant\n</code></pre>\n<p>You would run this macro from the package to create the external table—and, after creation, to update its partition metadata if you don't have <code>auto_refresh</code> enabled (see Snowflake <a href=\"https://docs.snowflake.com/en/sql-reference/sql/create-external-table.html\" rel=\"noreferrer\">docs</a>):</p>\n<pre><code>dbt run-operation stage_external_sources\n</code></pre>\n<p>You can then select from this source in an incremental model, like the one you have above. Now, <code>event_date</code> is a partition column on this external table, so filtering on it <strong>should</strong> enable Snowflake to prune files (though that's been inconsistent historically for dynamic, subquery-derived filters).</p>\n<pre><code>{{\n  config(\n    materialized ='incremental'\n  )\n}}\n \n  SELECT\n    metadata$filename as file_name,\n    event_date,\n    value as payload,\n    properties:mp_processing_time_ms::int / 1000 as event_timestamp_converted,\n    CONVERT_TIMEZONE('Europe/London', current_timestamp) as modeled_at\n\n from {{ source('s3_mixpanel', 'events' }} \n\n    \n{% if is_incremental() %}\n    -- this filter will only be applied on an incremental run\n    WHERE event_date &gt;(\n    SELECT\n        max(event_date)\n    FROM\n        {{ this }}\n    )\n{% endif %}\n\n{{ row_limit() }}\n</code></pre>\n"}
{"title":"dbt macro to iterate over item in list within a sql call?","question_body":"<p>First off, I am a dbt backer! I love this tool and the versatility of it.</p>\n<p>When reading some of the <a href=\"https://docs.getdbt.com/reference/dbt-jinja-functions/adapter#drop_relation\" rel=\"noreferrer\">docs</a> I noticed that I might be able to do some meta work on my schemas every time I call a macro.</p>\n<p>One of those would be to clean up schemas.</p>\n<p>(<em>This has been edited as per discussion within the dbt slack</em>)</p>\n<ol>\n<li><p><code>dbt run-operation freeze</code> that would introspect all of the tables that would be written with dbt run but with an autogenerated hash (might just be timestamp). It would output those tables in the schema of my choice and would log the “hash” to console.</p>\n</li>\n<li><p><code>dbt run-operation unfreeze --args '{hash: my_hash}'</code> that would then proceed to find the tables written with that hash prefix and clean them out of the schema.</p>\n</li>\n</ol>\n","answer_body":"<p>I have created such a macro in an older version of dbt and it still works on 0.17.1.</p>\n<p>The macro below <code>item_in_list_query</code> is getting a list of <code>tables</code> from a separate macro <code>get_tables</code> (also below). That list of tables is then concatenated inside <code>item_in_list_query</code> to compose a desired SQL query and execute it. For demonstration there is also a model in which <code>item_in_list_query</code> is used.</p>\n<h3>item_in_list_query</h3>\n<pre><code>{% macro item_in_list_query() %}\n\n    {% set tables = get_tables() %}\n\n    {{ log(&quot;Tables: &quot; ~ tables, True) }}\n\n    {% set query %}\n        select id\n        from my_tables\n        {% if tables -%}\n            where lower(table_name) in {% for t in tables -%} {{ t }} {%- endfor -%}\n        {%- endif -%}\n    {% endset %}\n\n    {{ log(&quot;query: &quot; ~ query, True) }}\n\n    {# run_query returns agate.Table (https://agate.readthedocs.io/en/1.6.1/api/table.html). #}\n    {% set results = run_query(query) %}\n\n    {{ log(&quot;results: &quot; ~ results, True) }}\n\n    {# execute is a Jinja variable that returns True when dbt is in &quot;execute&quot; mode i.e. True when running dbt run but False during dbt compile. #}\n    {% if execute %}\n    {# agate.table.rows is agate.MappedSequence in which data that can be accessed either by numeric index or by key. #}\n    {% set results_list = results.rows %}\n    {% else %}\n    {% set results_list = [] %}\n    {% endif %}\n\n    {{ log(&quot;results_list: &quot; ~ results_list, True) }}\n    {{ return(results_list) }}\n\n{% endmacro %}\n\n</code></pre>\n<h3>get_tables</h3>\n<pre><code>{% macro get_tables() %}\n      {%- set tables = [\n          ('table1', 'table2')\n      ] -%}\n  {{return(tables )}}\n{% endmacro %}\n\n</code></pre>\n<h3>model</h3>\n<pre><code>{%- for item in item_in_list_query() -%}\n  {%- if not loop.first %} UNION ALL {% endif %}\n  select {{ item.id }}\n{%- endfor -%}\n\n</code></pre>\n"}
{"title":"How to declare and init variable in a dbt model in `.sql` file with big query adaptor?","question_body":"<p>I would like to declare and init a variable in a dbt model <code>customer.sql</code> file.\nI used the keyword <code>DECLARE</code> to declare a variable like the <a href=\"https://cloud.google.com/bigquery/docs/reference/standard-sql/scripting\" rel=\"noreferrer\">BigQuery documentation</a> suggests but I got\na <code>Syntax error</code> on <code>DECLARE</code> keyword.</p>\n<p>Code:</p>\n<pre class=\"lang-sql prettyprint-override\"><code>DECLARE myDate VARCHAR DEFAULT '2021-01-01';\n\nwith order_bis as (\n\n    select\n        order_id\n\n    from\n        order\n    where\n        customer_date &gt; myDate\n\n)\n\nselect * from order_bis\n</code></pre>\n<p>Error:\n<code>Syntax error: Expected &quot;(&quot; or keyword SELECT or keyword WITH but got keyword DECLARE ...</code></p>\n","answer_body":"<p>It seems that using SQL variables does not work &quot;yet&quot; with dbt.\nYou can use Jinja variable if you want to use static values in multiple places, so that you can rely on Jinja logic.</p>\n<pre><code>{% set myVar = '2017-01-01' %}\n\n...\n\nwhere\n        customer_date &gt; {{myVar}}\n\n...\n</code></pre>\n"}
{"title":"Can we call any external REST API inside DBT(Data Build Tool)?","question_body":"<p>I am working on some analytical work and we need to transform data from one source to another and we are using <a href=\"https://www.getdbt.com/\" rel=\"nofollow noreferrer\">DBT</a> for transformation purpose. one of the data available to use via only REST API. so my question is can we call external API inside dbt file and extract the fields from its response. Do we have something?</p>\n","answer_body":"<p>Quoting from the founder's blog post: <a href=\"https://blog.getdbt.com/what--exactly--is-dbt-/\" rel=\"noreferrer\">&quot;What, exactly, is dbt?&quot;</a>,</p>\n<blockquote>\n<p>&quot;dbt is the T in ELT. It doesn’t extract or load data, but it’s\nextremely good at transforming data that’s already loaded into your\nwarehouse. This “transform after load” architecture is becoming known\nas ELT (extract, load, transform).&quot;</p>\n</blockquote>\n<p>Consequently, unless you already have the api response IN your warehouse, dbt won't be able to help you. You'll probably need an ELT engine (Stitch, Fivetran, Airflow etc. to name a few) to retrieve and store the API response. However, if you have the API response stored as say, a JSON object or a nested string - dbt can work with that.</p>\n<p>It may seem a little underwhelming but the magic of a great product sometimes is it's focus on being really, really great at just one thing.</p>\n<p>Edit 2022: If you are using <code>dbt</code> on a database that supports http or curl function calls and insist on taking this route, I recommend the following <a href=\"https://stackoverflow.com/questions/17407338/how-can-i-make-http-request-from-sql-server\">question</a> as a starting point. Good luck.</p>\n"}
