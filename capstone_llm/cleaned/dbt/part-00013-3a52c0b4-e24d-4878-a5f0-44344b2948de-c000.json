{"title":"Running DBT within Airflow through the Docker Operator","question_body":"<p>Building my question on <a href=\"https://stackoverflow.com/questions/64890144/how-to-run-dbt-in-airflow-without-copying-our-repo\">How to run DBT in airflow without copying our repo</a>, I am currently running airflow and syncing the dags via git. I am considering different option to include DBT within my workflow. One suggestion by <a href=\"https://stackoverflow.com/users/3823815/louis-guitton\">louis_guitton</a> is to Dockerize the DBT project, and run it in Airflow via the <a href=\"https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html\" rel=\"noreferrer\">Docker Operator</a>.</p>\n<p>I have no prior experience using the Docker Operator in Airflow or generally DBT. I am wondering if anyone has tried or can provide some insights about their experience incorporating that workflow, my main questions are:</p>\n<ol>\n<li>Should DBT as a whole project be run as one Docker container, or is it broken down? (for example: are tests ran as a separate container from dbt tasks?)</li>\n<li>Are logs and the UI from DBT accessible and/or still useful when run via the Docker Operator?</li>\n<li>How would partial pipelines be run? (example: wanting to run only a part of the pipeline)</li>\n</ol>\n","answer_body":"<p>Judging by your questions, you would benefit from trying to dockerise dbt on its own, independently from airflow. A lot of your questions would disappear. But here are my answers anyway.</p>\n<ol>\n<li>\n<blockquote>\n<p>Should DBT as a whole project be run as one Docker container, or is it broken down? (for example: are tests ran as a separate container from dbt tasks?)</p>\n</blockquote>\n</li>\n</ol>\n<p>I suggest you build one docker image for the entire project. The docker image can be based on the python image since dbt is a python CLI tool. You then use the CMD arguments of the docker image to run any dbt command you would run outside docker.\nPlease remember the syntax of <code>docker run</code> (which has nothing to do with dbt): you can specify any COMMAND you wand to run at invocation time</p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\n</code></pre>\n<p>Also, the first hit on Google for &quot;docker dbt&quot; is <a href=\"https://github.com/davidgasquez/dbt-docker/blob/master/Dockerfile\" rel=\"noreferrer\">this dockerfile</a> that can get you started</p>\n<ol start=\"2\">\n<li>\n<blockquote>\n<p>Are logs and the UI from DBT accessible and/or still useful when run via the Docker Operator?</p>\n</blockquote>\n</li>\n</ol>\n<p>Again, it's not a dbt question but rather a docker question or an airflow question.</p>\n<p>Can you see the logs in the airflow UI when using a DockerOperator? Yes, <a href=\"https://marclamberti.com/blog/how-to-use-dockeroperator-apache-airflow/\" rel=\"noreferrer\">see this how to blog post with screenshots</a>.</p>\n<p>Can you access logs from a docker container? Yes, Docker containers emit logs to <code>stdout</code> and <code>stderr</code> output streams (which you can see in airflow, since airflow picks this up). But logs are also stored in JSON files on the host machine in a folder <code>/var/lib/docker/containers/</code>. If you have any advanced needs, you can pick up those logs with a tool (or a simple BashOperator or PythonOperator) and do what you need with it.</p>\n<ol start=\"3\">\n<li>\n<blockquote>\n<p>How would partial pipelines be run? (example: wanting to run only a part of the pipeline)</p>\n</blockquote>\n</li>\n</ol>\n<p>See answer 1, you would run your docker dbt image with the command</p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run my-dbt-image dbt run -m stg_customers\n</code></pre>\n"}
